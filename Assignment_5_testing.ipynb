{"cells":[{"cell_type":"markdown","metadata":{"id":"lJw61-LjsXis"},"source":["# Deep Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"RsKeviKysXit"},"source":["For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will be testing the Q-Learning algorithm in the __\"Highway Environment\"__. In this environment our Q-Learning agent will operate a green car on a highway with other cars. The agent is penalized for hitting other cars and rewarded for driving faster."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZzQ8yjVdi3E","outputId":"bfbe2171-0bbf-4323-f5ab-4ad14f35b847"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [1\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [C\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:7 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n","Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,204 kB]\n","Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,011 kB]\n","Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n","Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,474 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,279 kB]\n","Fetched 5,927 kB in 2s (2,817 kB/s)\n","Reading package lists... Done\n"]}],"source":["# Download needed libraries\n","# If running outside of Google Colab, these lines may not be needed\n","# Install environment and libraries to render environment\n","!sudo apt-get update\n","!pip install git+https://github.com/DLR-RM/stable-baselines3 &> /dev/null\n","!pip install highway-env &> /dev/null\n","!pip install xvfbwrapper &> /dev/null\n","!apt-get install -y xvfb python-opengl x11-utils ffmpeg build-essential python-dev swig python-pygame &> /dev/null\n","!pip install PyVirtualDisplay &> /dev/null\n","\n","# install dependencies\n","!apt install chromium-browser xvfb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0YZeBsisXiv"},"outputs":[],"source":["# #############################################################################\n","# # TODO: Edit the path to your assignment folder                             #\n","# #############################################################################\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(\"/content/gdrive/My Drive/CIVE 6358/Assignment 5\")\n","# #############################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgSM1zJ0dnaz"},"outputs":[],"source":["#importing needed libraries\n","#DO NOT ADD OR MODIFY THIS CELL\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display\n","from gym.wrappers import record_video\n","from pathlib import Path\n","import base64\n","import gym\n","import highway_env\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","import os\n","import gymnasium as gym\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{"id":"aGhKhGFblXci"},"source":["In the following cells you will be shown the observations from the environment that will be fed into our Q-Learning agent. We give a stack of 4 images to the agent. By passing stacks of images, we are able to give the agent information about the velocity of the agent and other cars in the environment.\n","<br> __No modification of this cell is needed__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpIOsGGLdqx5"},"outputs":[],"source":["#DO NOT ADD OR MODIFY THIS CELL\n","env=gym.make('highway-fast-v0', render_mode='rgb_array')\n","config = {\n","      \"observation\": {\n","          \"type\": \"GrayscaleObservation\",\n","          \"observation_shape\": (128, 64),\n","          \"stack_size\": 4,\n","          \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n","          \"scaling\": 1.75,\n","      },\n","      \"policy_frequency\": 2\n","  }\n","env.configure(config)\n","env.reset()\n","for _ in range(4):\n","    obs,_,_,_,_ = env.step(env.action_type.actions_indexes[\"FASTER\"])\n","\n","    _, axes = plt.subplots(ncols=4, figsize=(12, 5))\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"LQGQO0vWlwRj"},"source":["**What does this cell do?**\n","\n","This cell will show an agent taking __random actions__ in the environment. The green car is the agent.\n","\n","__By default, the cell is commented out.__\n","\n","**********************************************\n","**Optional Instructions for Running the Cell**\n","\n","1. When running this cell for the first time, execute it to generate a sample video where the agent takes random actions.\n","\n","2. After running the cell and viewing the video, please comment out (disable) this cell.\n","\n","3. Restart the runtime by selecting \"Runtime\" -> \"Restart runtime\" from the top menu.\n","\n","4. Once the runtime is restarted, rerun the notebook starting from the beginning.\n","\n","\n","**Why Is This Important?**\n","\n","Rendering videos consumes memory, and in the unpaid version of Colab, rendering both the sample and final videos in a single runtime session can lead to a crash due to memory limitations. To prevent this, it's important to comment out this cell after viewing the sample video, restart the runtime, and then rerun the code. This ensures that enough memory is available for rendering the final video.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMK1ZyyHd8pk"},"outputs":[],"source":["# import gymnasium as gym\n","# #DO NOT ADD OR MODIFY THIS CELL\n","# def record_videos(env, path=\"videos\"):\n","#     wrapped = record_video.RecordVideo(env, path, force=True, video_callable=lambda episode: True)\n","#     # Capture intermediate frames\n","#     env.unwrapped.set_record_video_wrapper(wrapped)\n","\n","#     return wrapped\n","\n","# def show_videos(path=\"videos/random_actions\"):\n","#     html = []\n","#     for mp4 in Path(path).glob(\"*.mp4\"):\n","#         video_b64 = base64.b64encode(mp4.read_bytes())\n","#         html.append('''<video alt=\"{}\" autoplay\n","#                       loop controls style=\"height: 400px;\">\n","#                       <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","#                  </video>'''.format(mp4, video_b64.decode('ascii')))\n","#     ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n","# from tqdm.notebook import trange\n","# env = gym.make(\"highway-fast-v0\", render_mode = 'rgb_array')\n","# env.configure({\n","#     \"observation\": {\n","#         \"type\": \"GrayscaleObservation\",\n","#         \"observation_shape\": (128, 64),\n","#         \"stack_size\": 4,\n","#         \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n","#         \"scaling\": 1.75,\n","#     },\n","# })\n","\n","# from pyvirtualdisplay import Display\n","# display = Display(visible=0, size=(400, 300))\n","# display.start()\n","# from gym.wrappers import RecordVideo\n","\n","# def record_videos(env, path=\"videos/random_actions\"):\n","#     wrapped = RecordVideo(env, path)\n","\n","#     # Capture intermediate frames\n","#     env.unwrapped.set_record_video_wrapper(wrapped)\n","\n","#     return wrapped\n","\n","\n","# env = record_videos(env)\n","# for episode in trange(3, desc=\"Test episodes\"):\n","#     obs, done = env.reset()[0], False\n","#     while not done:\n","#       action = env.action_space.sample()\n","#       # print(action)\n","#       print(env.step(int(action)))\n","#       obs, reward, done, info = env.step(int(action))\n","#       #obs, reward, done, info = env.step(env.action_type.actions_indexes[\"FASTER\"])\n","# env.close()\n","# show_videos()\n","# display.stop()"]},{"cell_type":"markdown","metadata":{"id":"eYnt2_fFl7bl"},"source":["In this cell we define a small nueral network that we will use to aproximate the Q-Function.\n","<br> __No modification of this cell is needed__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFmm2KIFduqV"},"outputs":[],"source":["#DO NOT ADD OR MODIFY THIS CELL\n","import torch\n","from torch import nn\n","class Network(torch.nn.Module):\n","  def __init__(self):\n","      super(Network, self).__init__()\n","      n_input_channels = 4\n","      self.cnn = nn.Sequential(\n","          nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n","          nn.ReLU(),\n","          nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","          nn.ReLU(),\n","          nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","          nn.ReLU(),\n","          nn.Flatten(),\n","          nn.Linear(3072,3072),\n","          nn.ReLU(),\n","          nn.Linear(3072,3072),\n","          nn.ReLU(),\n","          nn.Linear(3072,5),\n","      )\n","  def forward(self,x):\n","    return self.cnn(x)\n"]},{"cell_type":"markdown","metadata":{"id":"CmnOXXhOmEe4"},"source":["Experience replay implementation\n","<br> __No modification of this cell is needed__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKa-uTdEdwCT"},"outputs":[],"source":["import collections\n","import numpy as np\n","\n","#datastructure for 1 experience\n","Experience = collections.namedtuple(\"Experience\", field_names=(\"state\",\"action\", \"reward\", \"done\", \"new_state\"))\n","\n","#store and sample multiple experiences of type Experience\n","class ExperienceReplay:\n","  def __init__(self, capacity):\n","      self.buffer = collections.deque(maxlen=capacity)\n","\n","  def __len__(self):\n","      return len(self.buffer)\n","\n","  def append(self, experience):\n","      self.buffer.append(experience)\n","\n","  def sample(self, batch_size):\n","    with torch.no_grad():\n","      indices = np.random.choice(len(self.buffer), batch_size,replace=False)\n","      states=[]\n","      actions=[]\n","      rewards=[]\n","      dones=[]\n","      next_states=[]\n","      for idx in indices:\n","        s,a,r,d,n=self.buffer[idx]\n","        if len(s) == 2:\n","          s = s[0]\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","        dones.append(d)\n","        next_states.append(n)\n","\n","      # Make sure the sequences have consistent lengths along the second dimension\n","      states = torch.Tensor(states)\n","      actions = torch.Tensor(actions)\n","      rewards = torch.Tensor(rewards).type(torch.float64)\n","      dones = torch.Tensor(dones).type(torch.float64)\n","      next_states = torch.Tensor(next_states)\n","\n","      # Now, you can return these tensors\n","      return states, actions, rewards, dones, next_states\n"]},{"cell_type":"markdown","metadata":{"id":"4RX9Dm-CmKRl"},"source":["<br> __Modify only the sections indicated__\n","\n","<br> A class called Agent is defined below that implements a Q-Learning agent:\n","<br>This class implements the following functions:\n","<br>fill_buffer(): Fills the buffer with a selected amount of state action pairs, the last episode during generation of state action pairs is saved\n","<br>rand_fill_buffer(): Fills the buffer by takeing random actions in the environment\n","<br>mean_reward(): calculates the mean reward achieved by the agent for a defined amount of episodes\n","<br>show_video(): function used to display video in notebook\n","<br>video(): record actions from agent and create videos\n","<br>load_save(): can be used to load a saved checkpoint to resume training, can be used to train a agent past the colab time limit.\n","You will need to implement the epsilon greed policy as well as the Q learning loss function.\n"]},{"cell_type":"markdown","metadata":{"id":"Zv3NklC_sXix"},"source":["# Only edit the code in the comment blocks \"TODO:\" is indicated. This appears in the fill_buffer() function where you will implement the epsilon greedy function and the train function where you will implement the loss function for Q-Learning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXmUK8i_dyDh"},"outputs":[],"source":["from torch._C import device\n","from pyvirtualdisplay import Display\n","from gym.wrappers import RecordVideo\n","\n","class Agent:\n","  def __init__(self, env, Network):\n","    self.env=env\n","    self.net=Network()\n","    self.target_net=Network()\n","    self.target_net_update_freq=50 #update target net every 50 iters\n","    self.buffer=ExperienceReplay(15000) #Size of Experience buffer is set to 15000 state action pairs\n","    self.fill_buffer_size=1000 #initial fill buffer, during training this is automatically set to 1 episode worth of state action pairs\n","    self.initial_buffer_exp=1000 #this controls how many random actions to take in the env to initially fill the buffer, the exact amount will vary as the last episode is rolled out to completion\n","    self.epsilon=1.0 #epsilon is initially set to 1.0 the slowly decayed\n","    self.eps_decay=.9999 #we decay epsilon by using the following formula epsilon=epsilon*eps_decay , this is automatically done no need to implement\n","    self.eps_min=0.02 #minimum epsilon value\n","    self.batch_size=10 #batch size, number of experiences we feed the Q-Learning Network during trainnig\n","\n","    #implementation  variables to store loss, reward, and set device\n","    self.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    self.net.to(self.device)\n","    self.target_net.to(self.device)\n","    self.loss_list=[]\n","    self.loss_list_idx=[]\n","    self.reward_list=[]\n","    self.reward_list_idx=[]\n","\n","    self.rand_fill_buffer()\n","    self.gamma=torch.tensor(0.99,dtype=torch.float64)\n","    isExist = os.path.exists(\"checkpoint\")\n","    if not isExist:\n","      os.makedirs(\"checkpoint\")\n","  #fills the replay buffer with state action pairs defined by self.fill_buffer_size\n","  #Epsilon greedy policy must be defined here\n","  def fill_buffer(self):\n","    steps=0\n","    not_full = True\n","    with torch.no_grad():\n","        while not_full:\n","          if steps>self.fill_buffer_size:\n","            not_full=False\n","          episode_done=False\n","          state=self.env.reset()[0]\n","\n","          while not episode_done:\n","            #############################################################################\n","            # TODO: Select action based on epsilon greed policy.                        #\n","            # Set the action variable from None to the correct action.                  #\n","            # The following variables can be used:                                      #\n","            # np.random.random() This is a random number from 0 to 1                    #\n","            # self.epsilon This is the epsilon value                                    #\n","            # random_action A random action that can be taken in the environment        #\n","            # agent_action The action the Q learning agent would take                   #\n","            #############################################################################\n","\n","            if np.random.random() < self.epsilon:\n","                action = self.env.action_space.sample()  # Random action\n","            else:\n","                state_tensor = torch.tensor(state, device=self.device, dtype=torch.float32)\n","                state_tensor = state_tensor.unsqueeze(0)  # Add batch dimension\n","                q_values = self.net(state_tensor)  # Get Q-values for each action\n","                _, action = torch.max(q_values, dim=1)  # Select action with max Q-value\n","                action = action.item()  # Get the actual action to take\n","\n","            #############################################################################\n","            self.epsilon = max(self.epsilon*self.eps_decay, self.eps_min)\n","            new_state, reward, is_done, truncated, _ = self.env.step(action)\n","            len(new_state)\n","            exp = Experience(state,action,reward,is_done or truncated,new_state)\n","            self.buffer.append(exp)\n","            steps+=1\n","            #pbar2.update(1)\n","            episode_done=is_done or truncated\n","            state=new_state\n","\n","#fill buffer with random actions defined by self.initial_buffer_exp\n","  def rand_fill_buffer(self):\n","    steps=0\n","    not_full = True\n","    print(\"filling buffer with random actions\")\n","    with tqdm(total=self.initial_buffer_exp) as pbar:\n","      while not_full:\n","          if steps>self.initial_buffer_exp:\n","            not_full=False\n","          # get experience from 1 episode\n","          episode_done=False\n","          state=self.env.reset()[0]\n","\n","          while not episode_done:\n","            #random action\n","            action = self.env.action_space.sample()\n","            new_state, reward, is_done, truncated, _ = self.env.step(action)\n","            #exp = Experience(state,action,reward,is_done,new_state)\n","            exp=[state,action,reward,is_done or truncated,new_state]\n","            self.buffer.append(exp)\n","            steps+=1\n","            pbar.update(1)\n","            #print(steps)\n","            episode_done=is_done or truncated\n","            state=new_state\n","\n","#calculate mean reward for 20 episodes\n","  def mean_reward(self):\n","    #average reward selected number of episodes\n","    rewards=0\n","    episodes=20\n","    ep_len=1\n","    for ep in range(episodes):\n","        episode_done=False\n","        state=torch.tensor(self.env.reset()[0]).type(torch.float32)\n","        while not episode_done:\n","          out=torch.unsqueeze(torch.tensor(state),0)\n","          action=self.net(out.to(self.device))\n","          action=int(torch.max(action,dim=1)[1])\n","          new_state, reward, is_done, truncated, _ = self.env.step(action)\n","          ep_len+=1\n","          episode_done=is_done or truncated\n","          rewards+=reward\n","          state=torch.Tensor(new_state).type(torch.float32)\n","\n","    print(\"average episode time steps: \"+str(ep_len/episodes))\n","    return rewards/episodes\n","\n","#used to load video in notebook\n","  def show_videos(self,path=\"videos/agent_actions\"):\n","    html = []\n","    for mp4 in Path(path).glob(\"*.mp4\"):\n","        video_b64 = base64.b64encode(mp4.read_bytes())\n","        html.append('''<video alt=\"{}\" autoplay\n","                      loop controls style=\"height: 400px;\">\n","                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","                 </video>'''.format(mp4, video_b64.decode('ascii')))\n","    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n","\n","#record and display video of agent taking actions in environment\n","  def video(self):\n","    display = Display(visible=0, size=(400, 300))\n","    display.start()\n","    def record_videos(env, path=\"videos/agent_actions\"):\n","        wrapped = RecordVideo(env, path)\n","\n","        # Capture intermediate frames\n","        env.unwrapped.set_record_video_wrapper(wrapped)\n","\n","        return wrapped\n","    env = gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n","    env.configure({\n","        \"observation\": {\n","            \"type\": \"GrayscaleObservation\",\n","            \"observation_shape\": (128, 64),\n","            \"stack_size\": 4,\n","            \"weights\": [0.2989, 0.5870, 0.1140],\n","            \"scaling\": 1.75,\n","        },\n","    })\n","    env = record_videos(env)\n","    for episode in range(3):\n","        state, done = env.reset()[0], False\n","        state=torch.tensor(state).type(torch.float32)\n","        while not done:\n","          out=torch.unsqueeze(torch.tensor(state),0)\n","          action=self.net(out.to(self.device))\n","          action=int(torch.max(action,dim=1)[1])\n","          state, reward, done, info = env.step(int(action))\n","          state=torch.Tensor(state).type(torch.float32)\n","\n","    env.close()\n","    self.show_videos()\n","\n","#load saved checkpoint to extend training past 3000\n","  def load_save(self,save_path):\n","    self.net.load_state_dict(torch.load(save_path))\n","    self.target_net.load_state_dict(torch.load(save_path))\n","\n","#Used to train the Q-Learning Agent\n","#The loss function must be defined here\n","  def train(self,total_iterations):\n","    optimizer = torch.optim.Adam(self.net.parameters(),lr=5e-4)\n","    self.fill_buffer_size=1\n","    for idx in range(total_iterations):\n","\n","      #fill buffer with exp using model and epsilon greedy policy\n","      self.fill_buffer()\n","\n","      #We take a random batch from the replay buffer\n","      batch=self.buffer.sample(self.batch_size)\n","      #unpack batch\n","\n","\n","      #use these variable to calculate loss for Q function\n","      state,action,reward,is_done,next_state=batch\n","\n","      #############################################################################\n","      # TODO: Calculate Q value, target Q value, and Loss for Q Learning          #\n","      # refer to lecture 17 and the following links:                     #\n","      # https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html  #\n","      # https://arxiv.org/pdf/1312.5602v1.pdf                                     #\n","      # Note: Use Mean Squared Error for Loss, DO NOT use Huber Loss              #\n","      # use the following variables:                                              #\n","      # self.net() :This is the current netowkr used to aproximate the Q function #\n","      # self.target_net() :This is the target network used in the Q learning loss #\n","      # state :This is the current state                                          #\n","      # action :This is the action taken in the current state                     #\n","      # reward :This is the reward for taking the action in the current state     #\n","      #         and going to the next_state                                       #\n","      # is_done :Boolean value of 0 if not done and 1 if done                     #\n","      # next_state :next state for taking action in current state                 #\n","      # Note: all of these variables have a batch dimension first                 #\n","      #############################################################################\n","      # IMPORTANT:                                                                #\n","      # Please refer to Algorithm 1 in the DQN paper to understand                #\n","      # how to calculate the Loss for a terminal state, when done is set to True/1#\n","      #############################################################################\n","      #compute Q value\n","      #give state to net and select action\n","\n","      state = state.to(self.device, dtype=torch.float32)\n","      next_state = next_state.to(self.device, dtype=torch.float32)\n","      action = action.to(self.device, dtype=torch.long)\n","      reward = reward.to(self.device, dtype=torch.float32)\n","      is_done = is_done.to(self.device, dtype=torch.float32)\n","\n","      current_q_values = self.net(state).gather(1, action.unsqueeze(-1)).squeeze(-1)\n","\n","      #calculate expected value using target network\n","      #value if state is done is just the reward\n","\n","      next_q_values = self.target_net(next_state).max(1)[0]\n","      expected_q_values = reward + self.gamma * next_q_values * (1 - is_done)\n","      # print('next_q_values', next_q_values, 'expected_q_values', expected_q_values)\n","      loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values.detach())\n","\n","      #############################################################################\n","\n","      self.loss_list.append(loss.detach().clone().cpu() )\n","      self.loss_list_idx.append(idx+1)\n","      optimizer.zero_grad()\n","      loss.backward()\n","\n","      torch.nn.utils.clip_grad_norm_(self.net.parameters(), 10)\n","      optimizer.step()\n","\n","      if (idx+1)%self.target_net_update_freq==0:\n","        print(\"iteration count = \"+str(idx+1))\n","        print(\"update target net\")\n","        mean_rewards=self.mean_reward()\n","        self.reward_list.append(mean_rewards)\n","        self.reward_list_idx.append(idx+1)\n","        print(\"mean reward \"+str(mean_rewards))\n","        #update target net here\n","        self.target_net.load_state_dict(self.net.state_dict())\n","        #save model every 500 iterations\n","        if (idx+1)%500==0:\n","          torch.save(self.net.state_dict(), \"checkpoint/reward_\"+str(int(mean_rewards)) +\"_iteration_\"+ str(idx+1) + \"_model.pt\" )\n","    return\n"]},{"cell_type":"markdown","metadata":{"id":"PDO2RanFsXix"},"source":["Run the following cells to train your Q-Learning agent for 3000 iterations. A video will be saved in the videos/agent_actions folder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MGChjkYdztq"},"outputs":[],"source":["#We create an instance of the environment here\n","env = gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n","env.configure({\n","    \"observation\": {\n","        \"type\": \"GrayscaleObservation\",\n","        \"observation_shape\": (128, 64),\n","        \"stack_size\": 4,\n","        \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n","        \"scaling\": 1.75,\n","    },\n","})\n","\n","# We will create an object from the Agent class called Q_agent\n","# We use this object's train function to train our Agent and display a video of the trained Agent\n","Q_agent=Agent(env,Network)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeTTKUkvZysA"},"outputs":[],"source":["Q_agent.train(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZr8M1bcZyhe"},"outputs":[],"source":["Q_agent.video()"]},{"cell_type":"markdown","metadata":{"id":"UZdXXDAKsXiy"},"source":["The following cell will display the reward your agent achieved during training vs the iterations it has completed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJEvOlXevEwp"},"outputs":[],"source":["plt.plot(Q_agent.reward_list_idx,Q_agent.reward_list)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0Js238kUsXiy"},"source":["The following cell will display the loss calculated for your agent during training vs the iterations it has completed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WcNF5gvvL8S"},"outputs":[],"source":["plt.plot(Q_agent.loss_list_idx[1:],Q_agent.loss_list[1:])\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernel_info":{"name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":0}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mIUuNu6pVl3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695439761794,"user_tz":240,"elapsed":1950,"user":{"displayName":"Chan Min Park","userId":"00873252993035874207"}},"outputId":"f91c8058-6055-4d8d-f3a4-3e8076a2f156"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import os\n","os.chdir(\"/content/gdrive/My Drive/CIVE 6358/Assignment 2\")"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"LIHrWsYmVPUB"},"source":["# Implement a Neural Network\n","\n","This notebook contains useful information and testing code to help you to develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file."]},{"cell_type":"code","metadata":{"id":"RqU4g4PKVPUC","executionInfo":{"status":"error","timestamp":1695439717980,"user_tz":240,"elapsed":141,"user":{"displayName":"Chan Min Park","userId":"00873252993035874207"}},"colab":{"base_uri":"https://localhost:8080/","height":414},"outputId":"2a6d15ae-bda1-4025-dbc8-f25207ff1e08"},"source":["# import numpy as np\n","import cupy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import math\n","\n","from models import NeuralNetwork\n","\n","######### If not using Colab, you may skip these setup #########\n","# Set up plt for Colab\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# Set up Colab for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","####################### End of setup ###########################\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":20,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-80afdc213d0e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m######### If not using Colab, you may skip these setup #########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'NeuralNetwork' from 'models' (/content/gdrive/My Drive/CIVE 6358/Assignment 1/models/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"EAJL53ExAYFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([[0.2,-2],[3,-4]])\n","(X > 0) * 1"],"metadata":{"id":"G-s7k8goP98Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.mean(np.array([[1,2],[3,4]]))\n","np.maximum(0, np.array([[1,-2],[-3,4]]))"],"metadata":{"id":"DFaiAJcZQdyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([[1,2],[3,4]])\n","(np.exp(-X)/1+np.exp(-X))**2"],"metadata":{"id":"DCRZZVLoQfoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([[1,2],[3,4]])\n","(np.exp(-X)/(1+np.exp(-X)))**2"],"metadata":{"id":"MLLnzaKpQiLL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZ47zoSIVPUG"},"source":["You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. You will also need to implement the nonlinearity functions and their corresponding gradient functions in the `models/neural_net.py` file. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays.\n","\n","The cell below initializes a toy dataset and corresponding model which will allow you to check that your forward and backward pass by using a numeric gradient check."]},{"cell_type":"code","metadata":{"id":"qEpr7W8OVPUH"},"source":["# Create a small net and some toy data to check your implementations.\n","# Note that we set the random seed for repeatable experiments.\n","\n","input_size = 4\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","\n","\n","def init_toy_model(nonlinearity, num_layers):\n","    np.random.seed(0)\n","    return NeuralNetwork(icput_size, [hidden_size]*(num_layers-1), num_classes, num_layers, nonlinearity=nonlinearity)\n","\n","def init_toy_data():\n","    np.random.seed(1)\n","\n","    X = 10 * np.random.randn(num_inputs, input_size)\n","    y = np.array([0, 1, 2, 2, 1])\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMO0Vek6VPUK"},"source":["# Implement forward and backward pass\n","\n","The first thing you will do is implement the forward pass of your neural network along with the loss calculation. The forward pass should be implemented in the `forward` function and you will also need to complete the `sigmoid` and `relu` functions.\n","\n","Next, you will implement the loss and backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use a softmax fuction with cross entropy loss for loss calcuation. Fill in the code blocks in `NeuralNetwork.loss`."]},{"cell_type":"markdown","metadata":{"id":"rxCkJ4kLVPUK"},"source":["# Gradient  check\n","\n"," If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check:"]},{"cell_type":"code","metadata":{"id":"KKF1i_lqVPUL"},"source":["from utils.gradient_check import eval_numerical_gradient\n","\n","X, y = init_toy_data()\n","\n","# Use numeric gradient checking to check your implementation of the backward pass.\n","# If your implementation is correct, the difference between the numeric and\n","# analytic gradients should be around 1e-8 or less for each of the parameters\n","# W1, b1,... in your network.\n","\n","for net_activation in ['sigmoid', 'relu']:\n","    for num in [2, 3]:\n","        print(net_activation)\n","        net = init_toy_model(net_activation, num)\n","        loss, grads = net.loss(X, y, reg=0.05)\n","\n","        # these should all be less than 1e-8 or so\n","        for param_name in grads:\n","            f = lambda W: net.loss(X, y, reg=0.05)[0]\n","            param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","            print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wt7AmWIqVPUN"},"source":["# Train the network\n","To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers you trained. Look at the function `NeuralNetwork.train` and fill in the missing sections to implement the training procedure. This should be similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement `NeuralNetwork.predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n","\n","Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2 using a two-layer network with relu activation."]},{"cell_type":"code","metadata":{"id":"KX8aR9nbVPUO"},"source":["net = init_toy_model('relu', 2)\n","stats = net.train(X, y, X, y,\n","            learning_rate=1e-1, reg=5e-6,\n","            num_iters=100, verbose=False)\n","\n","print('Final training loss: ', stats['loss_history'][-1])\n","\n","# plot the loss history\n","plt.plot(stats['loss_history'])\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputsize=4\n","hidden_size=[2,3]\n","output_size=6\n","num_layers=3\n","sizes=[inputsize]+hidden_size+[output_size]\n","params={}\n","nonlinearity='sigmoid'\n","x=np.random.randn(10,inputsize)\n","y=np.random.random_integers(0,5,10)\n","\n","# vectorized_relu= np.vectorize(relu)\n","\n","for i in range(1, num_layers + 1):\n","            W=np.random.randn(sizes[i - 1], sizes[i]) / np.sqrt(sizes[i - 1])\n","            b=np.zeros(sizes[i])\n","            params['W' + str(i)] = W\n","            params['b' + str(i)] = b\n","# if nonlinearity == 'sigmoid':\n","#             nonlinear = sigmoid\n","#             nonlinear_grad = sigmoid_grad\n","# elif nonlinearity == 'relu':\n","#             nonlinear = relu\n","#             nonlinear_grad = relu_grad\n","scores = None\n","layer_output = {}\n","layer_output['h'+str(0)]=x\n","\n","print(layer_output)\n","# for i in range(1,num_layers+1):\n","#     inputlayer=layer_output['h'+str(i-1)]\n","#     layer_output['z'+str(i)]=np.dot(inputlayer, params['W' + str(i)]) + params['b' + str(i)]\n","#     layer_output['h'+str(i)]=vectorized_relu(layer_output['z'+str(i)])\n","\n","# scores=softmax(layer_output['h'+str(num_layers)])\n","# true_y_ohe = np.zeros(scores.shape)\n","# true_y_ohe[range(len(y)), y] = 1\n","\n","# Sum=0\n","# ##regularization sum of all weight and bias matrices.\n","# for i in range(1,num_layers+1):\n","#     w=params['W'+str(i)]\n","#     w2=w**2\n","#     b=params['b'+str(i)]\n","#     b2=b**2\n","#     Sum=np.sum(w2)+np.sum(b2)+Sum\n","\n","\n","# ce_loss = -np.sum(true_y_ohe*np.log(scores))/len(y)+Sum"],"metadata":{"id":"4-eGgutwTb_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A_l2 = np.array([[1,-2],[3,-4],[5,6]])\n","W_l2 = np.array([[1,-1],[2,-2],[3,-3]])\n","W_l1 = np.array([[10,10],[11,11],[12,12]])\n","learning_rate = 0.1\n","\n","print(A_l2@W_l1.transpose())\n","\n","# A_l1 = W_l2*((A_l2> 0) * 1)\n","# W_l1_new = W_l1 - learning_rate*A_l1\n","\n","# b_l2 = np.array([[6],[8]])\n","# b_l1_new = b_l2 - learning_rate*(1*((A_l2> 0) * 1))\n","\n","# print(W_l1_new)\n","# print(b_l1_new)"],"metadata":{"id":"uu4vPPC0dt5q"},"execution_count":null,"outputs":[]}]}